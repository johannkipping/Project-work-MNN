\documentclass[]{article}

%opening
\title{Project Documentation for \\Mathematics of Neural Networks}
\author{Johann Kipping}

\begin{document}

\maketitle

\begin{abstract}
The aim of this project is to test the learned things in the first half of the lecture series on Neural Networks on the example of the Fashion-MNIST Dataset\\
\\
Things that could be done on Fashion-MNIST
\begin{itemize}
	\item Hyperparameter search
	\item Activation functions
	\item Initialization
	\item Optimizers
	\item Regularization (Overfitting?) + Generality
	\item FFT/im2col/Winograd
	\item Attribution
	\item Feature Visualization
	\item Adversarial Attacks
	\item Going deep?
\end{itemize}
\end{abstract}

\section{Idea collection and planning}
\textbf{Hyperparameters:}\\
Idea: For every test, do several choices of hyperparameters to get a feel for the choices\\
\\
\textbf{Activation functions / network structure}\\
Idea: Test small network structures (fast trainable) for effect of structural changes and different choices of activation functions using knowledge from the lecture and Exercise 4 from sheet 6 as starting point for network evaluation.\\
\\
\textbf{Initialization}\\
Idea: Verify the validity of the three approaches discussed in the lecture. (Zero/Random/Known data)\\
\\
\textbf{Optimizers}\\
Idea: Use Adam for all computations to reduce complexity of project as it incorporates other approaches.\\
\\
\textbf{Regularization (Overfitting?) + Generality}\\
Idea: Test for overfitting and generalization with own pictures (for fun). Maybe try regularization if poor performance is observed.\\
\\
\textbf{FFT/im2col/Winograd}\\
Idea: Let tensorflow decide on the usage of different approaches as insight was achieved in the exercises and modifications are not cheap.\\
\\

Progress will be documented with screenshots of the console and explanation. Graphs will be used.
\end{document}
