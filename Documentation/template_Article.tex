\documentclass[]{article}

%opening
\title{Project Documentation for \\Mathematics of Neural Networks}
\author{Johann Kipping}

\begin{document}

\maketitle

\begin{abstract}
The aim of this project is to test the learned things in the first half of the lecture series on Neural Networks on the example of the Fashion-MNIST Dataset\\
\\
Things that could be done on Fashion-MNIST
\begin{itemize}
	\item Hyperparameter search
	\item Activation functions
	\item Initialization
	\item Optimizers
	\item Regularization (Overfitting?) + Generality
	\item FFT/im2col/Winograd
	\item Attribution
	\item Feature Visualization
	\item Adversarial Attacks
	\item Going deep?
\end{itemize}
\end{abstract}

\section*{Section 0: Getting TF to run with a graphics card with compute capability of 3.0}
As this is a documentation of work this will be included here as it took a lot of time

\section{Idea collection and planning}
This chapter aims to collect ideas to most of the topics that were discussed in the lecture and continues to formulate a plan on how to tackle them in this project work.

\label{ideas}
Loss functions used is categorical cross entropy. Why not Mean squared or something else

\textbf{Hyperparameters:}\\
Idea: Try several choices of hyperparameters to get a feel for the effects. Do this in a grid-like fashion.\\
\\
\textbf{Activation functions / network structure}\\
Idea: Test small network structures (fast trainable) for effect of structural changes and different choices of activation functions using knowledge from the lecture and Exercise 4 from sheet 6 as starting point for network evaluation.\\
\\
\textbf{Initialization}\\
Idea: Verify the validity of the three approaches discussed in the lecture. (Zero/Random/Known data)\\
\\
\textbf{Optimizers}\\
Idea: Use Adam for all computations to reduce complexity of project as it incorporates other approaches.\\
\\
\textbf{Regularization (Overfitting?) + Generality}\\
Idea: Test for overfitting and generalization with own pictures (for fun). Maybe try regularization if poor performance is observed.\\
\\
\textbf{FFT/im2col/Winograd}\\
Idea: Let tensorflow decide on the usage of different approaches as insight was achieved in the exercises and modifications are not cheap.\\
\\
\textbf{Attribution/Feature Visualization}\\
Idea: Visualize the computed nets feature maps.\\
\\

Progress will be documented with screenshots of the console and explanation. Graphs will be used.

\section{Getting to know Neural Networks}
This section will describe the first set of investigative experiments. Mainly regarding stuff of the first couple of lectures its main aim is to allow to get an understanding and feeling for the behavior of Neural Networks.

\subsection{Starting out}
As a starting point the network structure from Exercise 4 of sheet 6 was chosen to allow further investigation. As described in section \ref{ideas} Adam was be used as the optimizer throughout this section in order to reduce complexity.

\subsection{Initialization experiments}
The first 

\subsection{Network structure}
\subsection{Hyperparameters}


\section{Regularization and Attribution}
Check for generality and attribute in the same step by using an image from the test set and an image that was taken by me\\
\\
Using gradient tape -> saliency map\\
Global average pooling -> class activation map\\
Grad-CAM! am ehesten

\end{document}
