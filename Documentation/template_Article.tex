\documentclass{article}

\usepackage[a4paper, margin=1.5cm]{geometry}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{array}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multicol}
\setlength{\columnsep}{1cm}

%opening
\title{Project Documentation for \\Mathematics of Neural Networks}
\author{Johann Kipping}

\begin{document}

\maketitle

\begin{multicols}{2}

\begin{abstract}
The aim of this project is to test the learned things in the first half of the lecture series on Neural Networks on the example of the Fashion-MNIST Dataset\\
\\
Things that could be done on Fashion-MNIST
\begin{itemize}
	\item Hyperparameter search
	\item Activation functions
	\item Initialization
	\item Optimizers
	\item Regularization (Overfitting?) + Generality
	\item FFT/im2col/Winograd
	\item Attribution
	\item Feature Visualization
	\item Adversarial Attacks
	\item Going deep?
\end{itemize}
\end{abstract}

\section*{0\quad Getting TF to run with a GPU with CUDA CC of 3.0}
As this is a documentation of work it will be mentioned that a significant amount of time needed for the setup of computations was put into getting tensorflow to compute on the available GPU (Geforce 660 Ti). The major obstacle was the fact that this particular GPU has a \textit{CUDA compute capability} of 3.0. The least tensorflow supports as of now is 3.5. This lead to the necessity of \textbf{downgrading to tensorflow version 1.12}. This might impede the ability for coverage of advanced topics.

\section{Idea collection and planning}
\label{ideas}
This chapter aims to collect ideas to most of the topics that were discussed in the lecture and continues to formulate a plan on how to tackle them in this project work.\\
\\
Loss functions used is categorical cross entropy. Why not Mean squared or something else\\
\\
\textbf{Hyperparameters:}\\
Idea: Try several choices of hyperparameters to get a feel for the effects. Do this in a grid-like fashion.\\
\\
\textbf{Activation functions / network structure}\\
Idea: Test small network structures (fast trainable) for effect of structural changes and different choices of activation functions using knowledge from the lecture and Exercise 4 from sheet 6 as starting point for network evaluation.\\
\\
\textbf{Initialization}\\
Idea: Verify the validity of the three approaches discussed in the lecture. (Zero/Random/Known data)\\
\\
\textbf{Optimizers}\\
Idea: Use Adam for all computations to reduce complexity of project as it incorporates other approaches.\\
\\
\textbf{Regularization (Overfitting?) + Generality}\\
Idea: Test for overfitting and generalization with own pictures (for fun). Maybe try regularization if poor performance is observed.\\
\\
\textbf{FFT/im2col/Winograd}\\
Idea: Let tensorflow decide on the usage of different approaches as insight was achieved in the exercises and modifications are not cheap.\\
\\
\textbf{Attribution/Feature Visualization}\\
Idea: Visualize a net (with more layers) feature maps by using Saliency maps and grad-Cam. Try out guided backpropagation if possible! Relu has to be used! Gradient ascent (google feature visualization)!\\
Neuron/Channel/Layer(DeepDream)
\\
\textbf{Adversarial Attack}\\
Idea: Use an adversarial attack to trick a net.

\section{Getting to know Neural Networks}
This section will describe the first set of investigative experiments. Mainly regarding stuff of the first couple of lectures its main aim is to allow to get an understanding and feeling for the behavior of Neural Networks.

\subsection{Starting out}
As a starting point the network structure from Exercise 4 of sheet 6 was chosen to allow further investigation. As described in section \ref{ideas} Adam was be used as the optimizer throughout this section in order to reduce complexity.

\subsection{Initialization experiments}
The methods for initializations are to be tested in this section:
\begin{itemize}
	\item zero
	\item known data
	\item random
\end{itemize}
As mentioned before the net structure used for this will be structure from Exercise 4 of sheet 6.\\
The random initialization will be employed before the known data approach as the results from the random initialization will be used as the known data.
\subsubsection{Zero initialization}
As expected little to no improvement can be seen from training.
\subsubsection{Random initialization}
The baseline performance seen in the exercise. Uniform he initializer was used.
\subsubsection{Known data initialization}
This method allow the last two dense layers for classification to be further trained. This improved the networks performance for the training data, not for the test data

\begin{figure}[h!]
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../img_1_1_init/acc_plot_Zero_model.png}
		\caption{1a}
		\label{fig:sfig1}
	\end{subfigure}%
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../img_1_1_init/acc_plot_Random_model}
		\caption{1b}
		\label{fig:sfig2}
	\end{subfigure}
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../img_1_1_init/acc_plot_Known_data_model}
		\caption{1b}
		\label{fig:sfig2}
	\end{subfigure}
	\caption{plots of....}
	\label{fig:fig}
\end{figure}
The resulting argument to be made is that form now on random initialization will be used due to lacking of known data. Zero initialization is not to be used.

\subsection{Hyperparameters for baseline network}
The hyperparameters for the fixed network structure of the baseline network with Adam as a fixed optimizer are batch size, epochs, learning rate, activation functions and initializers. The latter two have been or will be examined at another point in this documentation. In the following part a small grid-like search is employed.\\
\\
\begin{centering}
	\begin{tabular}{c c c c}
		%\toprule[2pt]
		 & 32 & 64 & 128 \\
		\toprule[2pt]
		0.1 & x  & x & x \\
		\midrule[0.2pt]
		0.01 & x  & x & x \\
		\midrule[0.2pt]
		0.001 & x  & x & x \\
		\bottomrule[2pt]\\
		\multicolumn{4}{c}{Epochs: 5}
	\end{tabular}
	\hfill
	\begin{tabular}{c c c c}
		%\toprule[2pt]
		& 32 & 64 & 128 \\
		\toprule[2pt]
		0.1 & x  & x & x \\
		\midrule[0.2pt]
		0.01 & x  & x & x \\
		\midrule[0.2pt]
		0.001 & x  & x & x \\
		\bottomrule[2pt]\\
		\multicolumn{4}{c}{Epochs: 20}
	\end{tabular}
\end{centering}

Different training times\\
Curve appearance/saturation

\subsection{Network structure}


\section{Regularization and Attribution}
Check for generality and attribute in the same step by using an image from the test set and an image that was taken by me\\
\\
Using gradient tape -> saliency map\\
Global average pooling -> class activation map\\
Grad-CAM! am ehesten

\end{multicols}
\end{document}
